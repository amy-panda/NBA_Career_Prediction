{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a09faa8-353f-4088-8eb7-e8e2a0ae0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: hpsklearn in c:\\users\\yatindra\\appdata\\roaming\\python\\python39\\site-packages (0.1.0)\n",
      "Requirement already satisfied: nose in c:\\programdata\\anaconda3\\lib\\site-packages (from hpsklearn) (1.3.7)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from hpsklearn) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from hpsklearn) (1.9.1)\n",
      "Requirement already satisfied: hyperopt in c:\\users\\yatindra\\appdata\\roaming\\python\\python39\\site-packages (from hpsklearn) (0.2.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from hpsklearn) (1.0.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt->hpsklearn) (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt->hpsklearn) (4.64.1)\n",
      "Requirement already satisfied: py4j in c:\\users\\yatindra\\appdata\\roaming\\python\\python39\\site-packages (from hyperopt->hpsklearn) (0.10.9.7)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt->hpsklearn) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt->hpsklearn) (2.8.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt->hpsklearn) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->hpsklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->hpsklearn) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->hyperopt->hpsklearn) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "#pip install hpsklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818a2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hyperopt in c:\\users\\yatindra\\appdata\\roaming\\python\\python39\\site-packages (0.2.7)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (1.9.1)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (4.64.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (2.8.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\yatindra\\appdata\\roaming\\python\\python39\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->hyperopt) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb18ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# For Hyperparameter optimization\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "\n",
    "# Loading data\n",
    "process_test_data = SourceFileLoader('process_test_data', 'D:/UTS_MDSI/ADSI/Part A/src/data/process_test_data.py').load_module()\n",
    "visualize = SourceFileLoader('visualize', 'D:/UTS_MDSI/ADSI/Part A/src/visualization/visualize.py').load_module()\n",
    "\n",
    "# optimization to calculate stats and parameter optimization\n",
    "#use_kaggle_data = False\n",
    "run_feature_selection = False  # Select the best set of given number of features \n",
    "run_parameter_optimization = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f35d8-68cc-4157-a4cb-454c3d385e2d",
   "metadata": {},
   "source": [
    "Split train datasets for training (80%), testing (10%) and validation (10%) and normalize features using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56538e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train shape: (8000, 21)\n",
      "Concat shape: (8000, 20)\n",
      "Files written to: D:/UTS_MDSI/ADSI/Part A/processed/week3\n",
      "X_train shape: (6400, 19)\n",
      "y_train shape: (6400, 1)\n",
      "X_test shape: (800, 20)\n",
      "y_test shape: (800, 1)\n",
      "X_valid shape: (800, 20)\n",
      "y_valid shape: (800, 1)\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data:\n",
    "    X_train, y_train, X_test = process_test_data.load_kaggle_train_and_test_data('D:/UTS_MDSI/ADSI/Part A/raw/2022_train.csv', 'D:/UTS_MDSI/ADSI/Part A/raw/2022_test.csv')\n",
    "else:    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = \\\n",
    "    process_test_data.split_and_normalize('D:/UTS_MDSI/ADSI/Part A/raw/2022_train.csv', 'D:/UTS_MDSI/ADSI/Part A/processed/week3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70bc90",
   "metadata": {},
   "source": [
    "Check details of the data if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773201bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1086e3",
   "metadata": {},
   "source": [
    "Selecting features using sequential feature selection if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1e141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_feature_selection==True:\n",
    "    num_of_features_to_select = 13\n",
    "    features = process_test_data.sequential_feature_selection('D:/UTS_MDSI/ADSI/Part A/raw/train.csv', num_of_features_to_select) #['GP', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3P Made', '3PA', 'OREB', 'DREB', 'REB', 'BLK', 'TOV']\n",
    "    X_train = X_train[features]\n",
    "    # Appending Id column since it should be kept\n",
    "    features.append('Id')\n",
    "    X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fe867-667d-4865-ae47-eb6ce0781730",
   "metadata": {},
   "source": [
    "Running parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37df6ab0-d6a8-401f-98fe-134e0b417211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-identified best parameters if parameter optimization is not going to run\n",
    "hyp_params = {\n",
    "'colsample_bytree': 0.65,\n",
    "'grow_policy': 'lossguide',\n",
    "'learning_rate': 0.05,\n",
    "'max_delta_step': 7,\n",
    "'max_depth': 2, \n",
    "'min_child_weight': 5.0,\n",
    "'min_split_loss': 11,\n",
    "'subsample': 0.2,\n",
    "'booster': 'gbtree'     \n",
    "}\n",
    "\n",
    "def hyperparameter_tuning(space):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(**space, eval_metric=\"auc\", use_label_encoder=False) \n",
    "    acc = cross_val_score(xgboost, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "    return{'loss': 1-acc, 'status': STATUS_OK }\n",
    "\n",
    "if run_parameter_optimization==True:\n",
    "    booster = ['gbtree', 'gblinear', 'dart']\n",
    "    grow_policy = ['depthwise', 'lossguide']\n",
    "    \n",
    "    space = {\n",
    "    \"max_depth\": hp.choice('max_depth', range(5, 20, 1)),\n",
    "    \"learning_rate\": hp.quniform('learning_rate', 0.01, 0.5, 0.05),\n",
    "    \"min_child_weight\": hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    \"subsample\": hp.quniform('subsample', 0.1, 1, 0.05),\n",
    "    \"colsample_bytree\": hp.quniform('colsample_bytree', 0.1, 1.0, 0.05),\n",
    "    \"min_split_loss\": hp.choice('min_split_loss', range(0, 20, 1)),\n",
    "    \"max_delta_step\": hp.choice('max_delta_step', range(0, 10, 1)),\n",
    "    \"grow_policy\": hp.choice(\"grow_policy\", grow_policy),     \n",
    "    \"booster\": booster[0]     \n",
    "    }\n",
    "    \n",
    "    # Initialize trials object\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn=hyperparameter_tuning, space = space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    \n",
    "    hyp_params['max_depth'] = best['max_depth']\n",
    "    hyp_params['learning_rate'] = best['learning_rate']\n",
    "    hyp_params['min_child_weight'] = best['min_child_weight']\n",
    "    hyp_params['subsample'] = best['subsample']\n",
    "    hyp_params['colsample_bytree'] = best['colsample_bytree'] \n",
    "    hyp_params['min_split_loss'] = best['min_split_loss']\n",
    "    hyp_params['max_delta_step'] = best['max_delta_step']\n",
    "    hyp_params['grow_policy'] = grow_policy[best['grow_policy']]\n",
    "    hyp_params['booster'] = booster[0]\n",
    "    \n",
    "    print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73c16f-e5b8-4adf-a274-b71205accd7b",
   "metadata": {},
   "source": [
    "Training the xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a837069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yatindra\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\Yatindra\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.65,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=11, gpu_id=-1,\n",
       "              grow_policy='lossguide', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.05, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=7,\n",
       "              max_depth=2, max_leaves=0, min_child_weight=5.0,\n",
       "              min_split_loss=11, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto', ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(**hyp_params, use_label_encoder=False)\n",
    "\n",
    "# Converting column y values to 1d array\n",
    "xgboost.fit(X_train, y_train, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354c22b",
   "metadata": {},
   "source": [
    "Predicting using trained random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567ad90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns to train\n",
    "test_X = X_test.loc[:, 'GP':'TOV']\n",
    "# Selecting Ids for CSV\n",
    "test_X_Ids = X_test.loc[:,'Id']\n",
    "\n",
    "if use_kaggle_data==True:\n",
    "    # Predicting probabilities and selecting probability of class 1.\n",
    "    pred = xgboost.predict_proba(test_X)[:,1]  \n",
    "else:\n",
    "    # Predicting classes (1 or 0) for calculating accuracy\n",
    "    pred = xgboost.predict(test_X) \n",
    "    # Probabilities for calculating ROC\n",
    "    rf_probs = xgboost.predict_proba(test_X)[:,1]\n",
    "\n",
    "# Data frame with ID for csv writing. In Kaggle mode pred will contains probabilities and else contains classes\n",
    "result = pd.DataFrame(data = {'Id': test_X_Ids, 'TARGET_5Yrs': pred}) \n",
    "# Extracting values for calculating stats\n",
    "result_values = result[['TARGET_5Yrs']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee49e4",
   "metadata": {},
   "source": [
    "Saving the trainned model and writing result to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ec11c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/UTS_MDSI/ADSI/Part A/models/Yatin_xgboost.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgboost, \"D:/UTS_MDSI/ADSI/Part A/models/Yatin_xgboost.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c12746",
   "metadata": {},
   "source": [
    "Show stats related to performance of the model if not using Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7867f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output=data_test[['Id','TARGET_5Yrs']]\n",
    "result.to_csv('D:/UTS_MDSI/ADSI/Part A/submission/submission_yatinXGB_week3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214fb000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error: 16.625%\n",
      "ROC: 0.70114\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data==False:\n",
    "    visualize.show_random_forest_stats(xgboost, test_X, y_test, rf_probs)\n",
    "else:\n",
    "    result.to_csv(\"D:/UTS_MDSI/ADSI/Part A/submission/submission_yatinXGB_week3.csv\", index = False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9d985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
